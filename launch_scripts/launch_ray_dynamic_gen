#!/usr/bin/python3


import argparse
import asyncio
import json
import os
import time
from copy import deepcopy
from typing import Dict, List, Optional

import deepspeed
import pytest
import torch
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from ray.serve.experimental.llm.models.casual_lm import CausalLM
from ray.serve.experimental.llm.models.opt import OPT
from ray.serve.experimental.llm.policy import (
    QuotaBasedRequestSelectionPolicy,
    RequestSelectionPolicy,
    StaticBatchPolicy,
)
from ray.serve.experimental.llm.queue import InferenceRequest, RequestQueue
from ray.serve.experimental.llm.scheduler import InferenceScheduler, TransfomerTokenizer
from ray.serve.experimental.llm.types import SamplingParams
from ray.serve.experimental.llm.worker import InferenceWorker
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, pipeline

os.environ["TRANSFORMERS_CACHE"] = '/data/cache'


MAX_LEN = 512
GEN_LEN = 128

app = FastAPI()


class ImpatientStaticBatchPolicy(RequestSelectionPolicy):
    def __init__(
        self,
        batch_size: int,
    ):
        self.batch_size = batch_size

    def select_new_requests(
        self, in_process_requests: List[InferenceRequest], queue: RequestQueue, has_oom: bool
    ) -> List[InferenceRequest]:
        if in_process_requests:
            return []

        results = []
        while not queue.empty() and len(results) < self.batch_size:
            request = queue.peek()
            results.append(request)
            queue.pop()

        return results


class FastAPIServer:
    def __init__(self, policy, model_name, max_batch_total_tokens, static_batch_size):
        self.policy = policy
        self.model_name = model_name
        self.model_ready = asyncio.Event()
        self.init_model(max_batch_total_tokens, static_batch_size)

        self.waiting_requests = []
        self.responses = []
        self.start_event = asyncio.Event()
        self.end_event = asyncio.Event()
        self.waiting_expectation = None

        self.test_tokenizer = AutoTokenizer.from_pretrained(model_name)

    def init_model(self, max_batch_total_tokens, static_batch_size):
        print(f'Init model {max_batch_total_tokens=}')
        self._inference_mode_raii_guard = torch._C._InferenceMode(True)
        assert 'opt' in self.model_name.lower()

        if self.policy == "static":
            request_selection_policy = StaticBatchPolicy(static_batch_size)
        elif self.policy == "impatient_static":
            request_selection_policy = ImpatientStaticBatchPolicy(
                static_batch_size)
        elif self.policy == "quota":
            request_selection_policy = QuotaBasedRequestSelectionPolicy(
                max_batch_total_tokens=max_batch_total_tokens, max_waiting_tokens=7
            )
        else:
            raise ValueError(f"unknown policy {self.policy=}")

        def create_model():
            model = OPT(self.model_name)
            self.model_ready.set()
            return model

        self.scheduler = InferenceScheduler(
            tokenizer=TransfomerTokenizer(
                pretrained_model_name_or_path=self.model_name, padding_side="left"
            ),
            inference_worker_loader=lambda: InferenceWorker(create_model),
            request_selection_policy=request_selection_policy,
            request_queue=RequestQueue(),
            loop=None,
            inline=False,
        )

    async def generate(self, request_dict: Dict):

        prompt = request_dict['inputs']
        parameters = request_dict['parameters']

        assert 'reponse_len' in parameters
        assert 'prompt_len' in parameters

        max_new_tokens = parameters.get('reponse_len', GEN_LEN)
        sampling_params = SamplingParams(
            temperature=1.0,
            repetition_penalty=1.0,
            top_k=0,
            top_p=1.0,
            typical_p=1.0,
            do_sample=False,
            max_new_tokens=max_new_tokens,
            stop_sequences=[],
            ignore_eos_token=True,
            watermark=False,
            seed=42,
        )

        max_length = parameters.get('prompt_len', MAX_LEN)

        result = await self.scheduler.async_process_request(
            prompt, sampling_params, max_length=max_length)

        # calculate generated tokens.
        prompt_ids = self.test_tokenizer(prompt)['input_ids']
        output_ids = self.test_tokenizer(result)['input_ids']
        actual_ray_gen_len = len(output_ids)
        # print(f'prompt_ids {max_length=} {len(prompt_ids)=} {prompt_ids=}')
        # print(f'output_ids {max_new_tokens=} {len(output_ids)=} {output_ids=}')

        print(f'desired len {max_new_tokens} actual len {actual_ray_gen_len}')

        result = {
            'ray_gen_len': max_new_tokens,
            'generated_text': result,
        }
        return result

    async def is_ready(self):
        return self.model_ready.is_set()


@app.post("/generate")
async def generate_stream(request: Request):
    request_dict = await request.json()
    return await server.generate(request_dict)


@app.get("/is_ready")
async def is_ready(request: Request):
    return await server.is_ready()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, required=True)
    parser.add_argument(
        '--policy', choices=["impatient_static", "static", "quota"], default="quota")
    parser.add_argument('--model-name', type=str, required=True)

    parser.add_argument('--max-batch-total-tokens', type=int)
    parser.add_argument('--static-batch-size', type=int)
    args = parser.parse_args()

    if args.policy == 'quota':
        assert args.max_batch_total_tokens is not None
        assert args.static_batch_size is None

    if args.policy == 'static':
        assert args.max_batch_total_tokens is None
        assert args.static_batch_size is not None

    if args.policy == 'impatient_static':
        assert args.max_batch_total_tokens is None
        assert args.static_batch_size is not None

    server = FastAPIServer(args.policy, args.model_name,
                           args.max_batch_total_tokens, args.static_batch_size)

    uvicorn.run(app, host=os.getenv("BENCHMARK_IP"), port=args.port, log_level="info")
